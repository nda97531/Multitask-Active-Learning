{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1471,
     "status": "ok",
     "timestamp": 1617469444426,
     "user": {
      "displayName": "Đức Anh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj__uwgJPnV_bbazx9cfsZZmzMDKRMuz1IsbXR7Sw=s64",
      "userId": "05901412776980253231"
     },
     "user_tz": -420
    },
    "id": "BlLpGdHmOF_c",
    "outputId": "c5ed9636-ccc3-4043-d48a-f0454f2aa7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask\n",
      "/content/drive/My Drive/Python project/abnormal_activities\n",
      "Sat Apr  3 17:04:04 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as tr, torch.nn as nn, torch.utils.data as trdata\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "from model_n_data.dataset import *\n",
    "import utils\n",
    "import numpy_augment\n",
    "\n",
    "print(expe_dir)\n",
    "print(os.getcwd())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2013,
     "status": "ok",
     "timestamp": 1617469444975,
     "user": {
      "displayName": "Đức Anh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj__uwgJPnV_bbazx9cfsZZmzMDKRMuz1IsbXR7Sw=s64",
      "userId": "05901412776980253231"
     },
     "user_tz": -420
    },
    "id": "hug2H_tlf-g7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_set,\n",
    "          valid_set=None,\n",
    "          weights_save_name='torch_model',\n",
    "          only_save_best_of_best=True,\n",
    "          save_before_early_stop=False,\n",
    "          curve_save_name=None,\n",
    "          opt='adam',\n",
    "          learning_rate=1e-3,\n",
    "          weight_decay=0.,\n",
    "          batch_size=32,\n",
    "          max_epoch=100,\n",
    "          class_weight=None,\n",
    "          patience=10,\n",
    "          multi_gpu=False,\n",
    "          sample_weight_divide_factor=None,\n",
    "          ):\n",
    "    trainloader = trdata.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    if valid_set is not None:\n",
    "        testloader = trdata.DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss(reduction='none') if class_weight is None else nn.CrossEntropyLoss(weight=class_weight, reduction='none')\n",
    "\n",
    "    if opt == 'radam':\n",
    "        from torch_radam import RAdam\n",
    "        optimizer = RAdam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay, )\n",
    "    elif opt == 'adam':\n",
    "        optimizer = tr.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError('opt is adam or radam')\n",
    "\n",
    "    bestmetric = 0.\n",
    "    org_patience = patience\n",
    "    last_epoch_save_name = None\n",
    "\n",
    "    if multi_gpu:\n",
    "        num_gpu = tr.cuda.device_count()\n",
    "        if num_gpu > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "            print(f'Let\\'s use {num_gpu} GPUs!')\n",
    "        else:\n",
    "            print('Only 1 GPU is available :(')\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    if curve_save_name is not None:\n",
    "        curve_train_loss = []\n",
    "        curve_valid_loss = []\n",
    "        curve_train_f1 = []\n",
    "        curve_valid_f1 = []\n",
    "        curve_train_acc = []\n",
    "        curve_valid_acc = []\n",
    "\n",
    "    for epoch in range(1, max_epoch + 1):\n",
    "        time_start = time.time()\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0.\n",
    "        n_batch = 0\n",
    "        ypred_train = []\n",
    "        ytrue_train = []\n",
    "\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            n_batch += 1\n",
    "\n",
    "            data, label = batch\n",
    "\n",
    "            # extract components in label tensor:\n",
    "            # sample weight\n",
    "            sample_weight = label[:, 1].float()\n",
    "            sample_weight = sample_weight/sample_weight.mean()\n",
    "\n",
    "            # multi-task mask\n",
    "            multitask_mask = label[:, 2].bool()\n",
    "\n",
    "            # label\n",
    "            label = label[:, 0].long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            trainoutput0, trainoutput1 = model(data, multitask_mask=multitask_mask) # source output, target output\n",
    "\n",
    "            # print(f\"data: source: {data[~multitask_mask][0, 0]}; target: {data[multitask_mask]}\")\n",
    "            # print(f\"output shape: source: {trainoutput0.shape}, target: {trainoutput1.shape}\")\n",
    "            # print(sample_weight)\n",
    "            # print(multitask_mask.sum(), multitask_mask)\n",
    "            # input(\".../\")\n",
    "\n",
    "            loss0 = loss_function(trainoutput0, label[~multitask_mask]) * sample_weight[~multitask_mask]\n",
    "            loss1 = loss_function(trainoutput1, label[multitask_mask]) * sample_weight[multitask_mask]\n",
    "\n",
    "            loss = loss0.mean() + loss1.mean()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate trainloss and train ytrue, ypred\n",
    "            with tr.no_grad():\n",
    "                epoch_loss += loss\n",
    "                ypred_train.append(trainoutput1)\n",
    "                ytrue_train.append(label[multitask_mask])\n",
    "            if batch_idx == 0:\n",
    "                print(f\"batch sample weight: {np.unique(sample_weight.cpu(), return_counts=True)}\")\n",
    "\n",
    "        # epoch evaluation\n",
    "        with tr.no_grad():\n",
    "            epoch_loss /= n_batch\n",
    "            ytrue_train = tr.cat(ytrue_train, 0).cpu().view(-1).numpy()\n",
    "            ypred_train = tr.cat(ypred_train, 0).cpu().argmax(1).view(-1).numpy()\n",
    "\n",
    "            train_f1 = metrics.f1_score(ytrue_train, ypred_train, average='macro')\n",
    "            train_acc = metrics.accuracy_score(ytrue_train, ypred_train)\n",
    "\n",
    "            if valid_set is not None:\n",
    "                model.eval()\n",
    "\n",
    "                val_epoch_loss = 0.\n",
    "                ypred_valid = []\n",
    "                ytrue_valid = []\n",
    "                n_batch = 0\n",
    "\n",
    "                for testbatch in testloader:\n",
    "                    n_batch += 1\n",
    "                    testdata, testlabel = testbatch\n",
    "                    testlabel = testlabel.long()\n",
    "                    \n",
    "                    _, testoutput = model(testdata, tr.ones(len(testdata)).bool())\n",
    "                    val_loss = loss_function(testoutput, testlabel).mean()\n",
    "                    \n",
    "                    val_epoch_loss += val_loss\n",
    "                    ypred_valid.append(testoutput)\n",
    "                    ytrue_valid.append(testlabel)\n",
    "\n",
    "                val_epoch_loss /= n_batch\n",
    "                ytrue_valid = tr.cat(ytrue_valid, 0).cpu().view(-1).numpy()\n",
    "                ypred_valid = tr.cat(ypred_valid, 0).cpu()\n",
    "                ypred_valid = ypred_valid[:, :6]\n",
    "                ypred_valid = ypred_valid.argmax(1).view(-1).numpy()\n",
    "\n",
    "                val_f1 = metrics.f1_score(ytrue_valid, ypred_valid, average='macro')\n",
    "                val_acc = metrics.accuracy_score(ytrue_valid, ypred_valid)\n",
    "            else:\n",
    "                val_epoch_loss = epoch_loss\n",
    "                val_f1 = train_f1\n",
    "                val_acc = train_acc\n",
    "\n",
    "            duration = time.time() - time_start\n",
    "            print(\"\\nepoch %d, time: %ds\\n\"\n",
    "                  \"train loss: %.4f\\ttrain f1: %.4f\\t train acc: %.4f\\n\"\n",
    "                  \"  val loss: %.4f\\t  val f1: %.4f\\t   val acc: %.4f\"\n",
    "                  % (epoch, duration, epoch_loss, train_f1, train_acc, val_epoch_loss, val_f1, val_acc))\n",
    "\n",
    "            if curve_save_name is not None:\n",
    "                curve_train_loss.append(epoch_loss.item())\n",
    "                curve_valid_loss.append(val_epoch_loss.item())\n",
    "                curve_train_f1.append(train_f1)\n",
    "                curve_valid_f1.append(val_f1)\n",
    "                curve_train_acc.append(train_acc)\n",
    "                curve_valid_acc.append(val_acc)\n",
    "\n",
    "            # model checkpoint\n",
    "            if val_f1 > bestmetric:\n",
    "                epoch_name = \"%s_%d_%.6f\" % (weights_save_name, epoch, val_f1)\n",
    "                patience = org_patience\n",
    "                print(\n",
    "                    f\"f1 improved from {bestmetric} to {val_f1}, save to {epoch_name} ---------------------------------\")\n",
    "                tr.save(model.state_dict(), epoch_name)\n",
    "                bestmetric = val_f1\n",
    "\n",
    "                if last_epoch_save_name is not None and os.path.exists(last_epoch_save_name) and only_save_best_of_best:\n",
    "                    os.remove(last_epoch_save_name)\n",
    "                last_epoch_save_name = epoch_name\n",
    "\n",
    "            # early stopping\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience <= 0:\n",
    "                    if save_before_early_stop:\n",
    "                        epoch_name = \"%s_%d_%.6f\" % (weights_save_name, epoch, val_f1)\n",
    "                        tr.save(model.state_dict(), epoch_name + '-earlystopping')\n",
    "                    print('STOPPED-------------------------------------------')\n",
    "                    break\n",
    "\n",
    "    if curve_save_name is not None:\n",
    "        import pandas\n",
    "\n",
    "        training_curve = np.array([\n",
    "            curve_train_loss,\n",
    "            curve_valid_loss,\n",
    "            curve_train_f1,\n",
    "            curve_valid_f1,\n",
    "            curve_train_acc,\n",
    "            curve_valid_acc,\n",
    "        ]).T\n",
    "        cols = [\"train_loss\",\n",
    "                \"valid_loss\",\n",
    "                \"train_f1\",\n",
    "                \"valid_f1\",\n",
    "                \"train_acc\",\n",
    "                \"valid_acc\"]\n",
    "        training_curve = pandas.DataFrame(training_curve, columns=cols)\n",
    "        training_curve.to_csv(f\"{curve_save_name}.txt\", index=False, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1617469842509,
     "user": {
      "displayName": "Đức Anh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj__uwgJPnV_bbazx9cfsZZmzMDKRMuz1IsbXR7Sw=s64",
      "userId": "05901412776980253231"
     },
     "user_tz": -420
    },
    "id": "-q8sz7QHOKil"
   },
   "outputs": [],
   "source": [
    "def load_n_train(augmenter, lr, gamma, sample_weight_divide_factor=-1,\n",
    "                 weight_file_1=None, weight_file_2=None, weight_file=None, save_name='weight'):\n",
    "\n",
    "    # LOAD WISDM\n",
    "    data_folder = f'../Dataset/wisdm'\n",
    "\n",
    "    train_subjects = np.unique(np.linspace(1, 36, 6, False, dtype=int))\n",
    "    valid_subjects = np.setdiff1d(np.arange(1, 1+36), train_subjects)\n",
    "    print(f'num subjects valid/train: {len(valid_subjects)}/{len(train_subjects)}')\n",
    "    print('valid_subjects', valid_subjects)\n",
    "    print('train_subjects', train_subjects)\n",
    "\n",
    "    train_data_n_label = utils.load_dataset_single_file(\n",
    "        data_file=f'{data_folder}/data_ss.npy',\n",
    "        p_lb_file=f\"{data_folder}/p_lb_ss.npy\",\n",
    "        list_valid_subject_id=valid_subjects,\n",
    "        is_train_set=True,\n",
    "        data_channels=[3],\n",
    "        normalize=False,\n",
    "        # min_vals=[-8, -150],\n",
    "        # max_vals=[8, 150]\n",
    "    )\n",
    "\n",
    "    valid_data_n_label = utils.load_dataset_single_file(\n",
    "        data_file=f'{data_folder}/data_ss.npy',\n",
    "        p_lb_file=f\"{data_folder}/p_lb_ss.npy\",\n",
    "        list_valid_subject_id=valid_subjects,\n",
    "        is_train_set=False,\n",
    "        data_channels=[3],\n",
    "        normalize=False,\n",
    "        # min_vals=[-8, -150],\n",
    "        # max_vals=[8, 150]\n",
    "    )\n",
    "\n",
    "    traindata = train_data_n_label[0]\n",
    "    trainlabel = train_data_n_label[1]\n",
    "    validdata = valid_data_n_label[0]\n",
    "    validlabel = valid_data_n_label[1]\n",
    "\n",
    "    # SET WEIGHT PLACEHOLDER FOR INSTANCES FROM TARGET DOMAIN\n",
    "    pholder = np.ones(shape=[len(trainlabel), 3], dtype=float)\n",
    "    pholder[:, 0] = trainlabel\n",
    "    trainlabel = np.copy(pholder)\n",
    "\n",
    "    # LOAD MOBIACT V2\n",
    "    data_folder = '../Dataset/mobiact_v2'\n",
    "    \n",
    "    pick_data_n_label = utils.load_dataset_single_file(\n",
    "        data_file=f'{data_folder}/data_new.npy',\n",
    "        p_lb_file=f\"{data_folder}/p_lb_ss_new.npy\",\n",
    "        list_valid_subject_id=[],\n",
    "        is_train_set=True,\n",
    "        data_channels=[3, 3],\n",
    "        normalize=False,\n",
    "        lb_col_in_p_lb=1,\n",
    "        # min_vals=[-8, -150],\n",
    "        # max_vals=[8, 150]\n",
    "    )\n",
    "    pick_index = np.load('notebooks/MotionSense_active/mobiactv2_r2_pick_index_one-tenth.npy')\n",
    "    \n",
    "    pickdata = pick_data_n_label[0]\n",
    "    picklabel = pick_data_n_label[-1]\n",
    "\n",
    "    # SET WEIGHT FOR INSTANCES FROM SOURCE DOMAIN\n",
    "    pholder = np.ones(shape=[len(picklabel), 3], dtype=float)\n",
    "    pholder[:, 0] = picklabel\n",
    "    picklabel = np.copy(pholder)\n",
    "\n",
    "    # SET WEIGTH FOR SOURCE AND TARGET LABELS\n",
    "    total_len = len(trainlabel) + len(pick_index)\n",
    "    source_weight = (len(pick_index) / total_len) ** -1\n",
    "    target_weight = (len(trainlabel) / total_len) ** -1\n",
    "    \n",
    "    target_weight *= gamma\n",
    "\n",
    "    print(f\"source size: {len(pick_index)}, target size: {len(trainlabel)}, total size: {total_len}\")\n",
    "    print(f\"source weight, target weight: {source_weight}, {target_weight}\")\n",
    "\n",
    "    picklabel[:, 1] = source_weight\n",
    "    trainlabel[:, 1] = target_weight\n",
    "\n",
    "    # SET MULTI-TASK MASK\n",
    "    picklabel[:, 2] = 0 # source mask\n",
    "    trainlabel[:, 2] = 1 # target mask\n",
    "\n",
    "    # MERGE DATA OF SOURCE AND TARGET DOMAIN\n",
    "    traindata = np.concatenate([traindata, pickdata[pick_index]])\n",
    "\n",
    "    n_class_source = len(np.unique(picklabel[:, 0]))\n",
    "    n_class_target = len(np.unique(trainlabel[:, 0]))\n",
    "    print(f\"num class source: {n_class_source}; target: {n_class_target}\")\n",
    "    \n",
    "    trainlabel = np.concatenate([trainlabel, picklabel[pick_index]])\n",
    "\n",
    "    train_set = DatasetWindows(traindata, trainlabel, augment_rate=0.5, augmenter=augmenter)\n",
    "    valid_set = DatasetWindows(validdata, validlabel, augment_rate=0.)\n",
    "\n",
    "    from model_n_data.model_tcn_classifier import TCN\n",
    "    model = TCN(n_classes=[n_class_source, n_class_target],  # type:int\n",
    "                how_flatten=\"spatial attention gap\",\n",
    "                how_between_tcn_blocks=\"none\",\n",
    "                n_tcn_channels=(64,) * 6 + (128,) * 2,  # type: list, tuple\n",
    "                tcn_kernel_size=2,  # type:int\n",
    "                dilation_base=2,  # type:int\n",
    "                tcn_droprate=0.2,  # type: float\n",
    "                use_spatial_dropout=False,\n",
    "                n_fc_layers=1,\n",
    "                fc_droprate=0.8,  # type: float\n",
    "                use_init_batchnorm=True,\n",
    "                use_last_fc=True,\n",
    "                )\n",
    "    model.cuda()\n",
    "    if weight_file is not None:\n",
    "        print('load w', weight_file)\n",
    "        model.load_state_dict(tr.load(weight_file))\n",
    "\n",
    "    print('------------------------------')\n",
    "    # print(model)\n",
    "    print('------------------------------')\n",
    "    print(f\"params: {utils.torch_num_params(model)}\")\n",
    "    print(f\"train data: {traindata.shape}\")\n",
    "    print(f\"train label: {trainlabel.shape}\")\n",
    "    print(f\"val data: {validdata.shape}\")\n",
    "    print(f\"val label: {validlabel.shape}\")\n",
    "\n",
    "    print(traindata.max())\n",
    "    print(validdata.max())\n",
    "    print(traindata.min())\n",
    "    print(validdata.min())\n",
    "    print()\n",
    "\n",
    "    train(model,\n",
    "          train_set=train_set,\n",
    "          valid_set=valid_set,\n",
    "          weights_save_name=f'{expe_dir}/{save_name}',\n",
    "          only_save_best_of_best=True,\n",
    "          save_before_early_stop=False,\n",
    "          curve_save_name=None,\n",
    "          opt='adam',\n",
    "          learning_rate=lr,\n",
    "          weight_decay=0.,\n",
    "          batch_size=32,\n",
    "          max_epoch=100,\n",
    "          class_weight=None,\n",
    "          patience=10,\n",
    "          multi_gpu=False,\n",
    "          sample_weight_divide_factor=sample_weight_divide_factor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2010,
     "status": "ok",
     "timestamp": 1617469444978,
     "user": {
      "displayName": "Đức Anh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj__uwgJPnV_bbazx9cfsZZmzMDKRMuz1IsbXR7Sw=s64",
      "userId": "05901412776980253231"
     },
     "user_tz": -420
    },
    "id": "YQhT0Z-inAib"
   },
   "outputs": [],
   "source": [
    "# augmenter = augment.Augmenter(\n",
    "#         input_shape=[300,3],\n",
    "#         augmentation_apply_rate=1,\n",
    "\n",
    "#         max_rotate_x=20, max_rotate_y=20, max_rotate_z=20,\n",
    "#     )\n",
    "augmenter = numpy_augment.NumpyAugmenter(\n",
    "    input_shape=[300, 3],\n",
    "    max_num_transformations=3,\n",
    "    shuffle_transformations=False,\n",
    "\n",
    "    rotate_x_range=[0., 20.],\n",
    "    rotate_y_range=[0., 20.],\n",
    "    rotate_z_range=[0., 20.],\n",
    "    # magnitude_warp_sigma_range=[0., 0.1],\n",
    "    # nums_magnitude_warp_knots=[3,4],\n",
    "    # time_warp_sigma_range=[0., 0.2],\n",
    "    # nums_time_warp_knots=[3,4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498725,
     "status": "ok",
     "timestamp": 1617470347163,
     "user": {
      "displayName": "Đức Anh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj__uwgJPnV_bbazx9cfsZZmzMDKRMuz1IsbXR7Sw=s64",
      "userId": "05901412776980253231"
     },
     "user_tz": -420
    },
    "id": "9g4xmZaRtzPx",
    "outputId": "e86c67ab-2b48-4616-9dd5-5a19689ae744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num subjects valid/train: 30/6\n",
      "valid_subjects [ 2  3  4  5  7  8  9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29\n",
      " 31 32 33 34 35 36]\n",
      "train_subjects [ 1  6 12 18 24 30]\n",
      "source size: 11164, target size: 7553, total size: 18717\n",
      "source weight, target weight: 1.6765496237907558, 2.4780881768833574\n",
      "num class source: 16; target: 6\n",
      "------------------------------\n",
      "------------------------------\n",
      "params: 223047.0\n",
      "train data: (18717, 300, 3)\n",
      "train label: (18717, 3)\n",
      "val data: (41682, 300, 3)\n",
      "val label: (41682,)\n",
      "19.61\n",
      "20.04\n",
      "-19.61\n",
      "-19.68087\n",
      "\n",
      "batch sample weight: (array([0.81692415, 1.2074859 ], dtype=float32), array([17, 15]))\n",
      "\n",
      "epoch 1, time: 24s\n",
      "train loss: 3.8452\ttrain f1: 0.6808\t train acc: 0.7160\n",
      "  val loss: 1.3656\t  val f1: 0.6320\t   val acc: 0.6750\n",
      "f1 improved from 0.0 to 0.6320004718073585, save to /content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask/r1_auto_weight_gamma1_1_0.632000 ---------------------------------\n",
      "batch sample weight: (array([0.827018 , 1.2224056], dtype=float32), array([18, 14]))\n",
      "\n",
      "epoch 2, time: 24s\n",
      "train loss: 3.5127\ttrain f1: 0.8320\t train acc: 0.8450\n",
      "  val loss: 1.2147\t  val f1: 0.8227\t   val acc: 0.8274\n",
      "f1 improved from 0.6320004718073585 to 0.8226655484186426, save to /content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask/r1_auto_weight_gamma1_2_0.822666 ---------------------------------\n",
      "batch sample weight: (array([0.827018 , 1.2224056], dtype=float32), array([18, 14]))\n",
      "\n",
      "epoch 3, time: 24s\n",
      "train loss: 3.4011\ttrain f1: 0.8840\t train acc: 0.8891\n",
      "  val loss: 1.2543\t  val f1: 0.7750\t   val acc: 0.7854\n",
      "batch sample weight: (array([0.80707383, 1.1929262 ], dtype=float32), array([16, 16]))\n",
      "\n",
      "epoch 4, time: 24s\n",
      "train loss: 3.3228\ttrain f1: 0.9103\t train acc: 0.9139\n",
      "  val loss: 1.2310\t  val f1: 0.8020\t   val acc: 0.8103\n",
      "batch sample weight: (array([0.7880689, 1.1648352], dtype=float32), array([14, 18]))\n",
      "\n",
      "epoch 5, time: 24s\n",
      "train loss: 3.2656\ttrain f1: 0.9201\t train acc: 0.9233\n",
      "  val loss: 1.2032\t  val f1: 0.8371\t   val acc: 0.8392\n",
      "f1 improved from 0.8226655484186426 to 0.837101898470157, save to /content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask/r1_auto_weight_gamma1_5_0.837102 ---------------------------------\n",
      "batch sample weight: (array([0.89323837, 1.320285  ], dtype=float32), array([24,  8]))\n",
      "\n",
      "epoch 6, time: 24s\n",
      "train loss: 3.2223\ttrain f1: 0.9278\t train acc: 0.9304\n",
      "  val loss: 1.1773\t  val f1: 0.8651\t   val acc: 0.8654\n",
      "f1 improved from 0.837101898470157 to 0.8651344078653501, save to /content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask/r1_auto_weight_gamma1_6_0.865134 ---------------------------------\n",
      "batch sample weight: (array([0.83736426, 1.2376982 ], dtype=float32), array([19, 13]))\n",
      "\n",
      "epoch 7, time: 24s\n",
      "train loss: 3.1887\ttrain f1: 0.9357\t train acc: 0.9383\n",
      "  val loss: 1.1744\t  val f1: 0.8686\t   val acc: 0.8694\n",
      "f1 improved from 0.8651344078653501 to 0.8685959320171691, save to /content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask/r1_auto_weight_gamma1_7_0.868596 ---------------------------------\n",
      "batch sample weight: (array([0.80707383, 1.1929262 ], dtype=float32), array([16, 16]))\n",
      "\n",
      "epoch 8, time: 24s\n",
      "train loss: 3.1670\ttrain f1: 0.9415\t train acc: 0.9433\n",
      "  val loss: 1.1566\t  val f1: 0.8875\t   val acc: 0.8864\n",
      "f1 improved from 0.8685959320171691 to 0.8875053274888773, save to /content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask/r1_auto_weight_gamma1_8_0.887505 ---------------------------------\n",
      "batch sample weight: (array([0.76993847, 1.138037  ], dtype=float32), array([12, 20]))\n",
      "\n",
      "epoch 9, time: 24s\n",
      "train loss: 3.1358\ttrain f1: 0.9472\t train acc: 0.9493\n",
      "  val loss: 1.1688\t  val f1: 0.8757\t   val acc: 0.8753\n",
      "batch sample weight: (array([0.8588536, 1.2694613], dtype=float32), array([21, 11]))\n",
      "\n",
      "epoch 10, time: 25s\n",
      "train loss: 3.1262\ttrain f1: 0.9455\t train acc: 0.9472\n",
      "  val loss: 1.1427\t  val f1: 0.9007\t   val acc: 0.9007\n",
      "f1 improved from 0.8875053274888773 to 0.9006673800753555, save to /content/drive/My Drive/Python project/abnormal_activities/notebooks/WISDM_active_multitask/r1_auto_weight_gamma1_10_0.900667 ---------------------------------\n",
      "batch sample weight: (array([0.87001723, 1.2859621 ], dtype=float32), array([22, 10]))\n",
      "\n",
      "epoch 11, time: 25s\n",
      "train loss: 3.1189\ttrain f1: 0.9492\t train acc: 0.9511\n",
      "  val loss: 1.1688\t  val f1: 0.8764\t   val acc: 0.8746\n",
      "batch sample weight: (array([0.827018 , 1.2224056], dtype=float32), array([18, 14]))\n",
      "\n",
      "epoch 12, time: 24s\n",
      "train loss: 3.1065\ttrain f1: 0.9516\t train acc: 0.9534\n",
      "  val loss: 1.1587\t  val f1: 0.8853\t   val acc: 0.8846\n",
      "batch sample weight: (array([0.827018 , 1.2224056], dtype=float32), array([18, 14]))\n",
      "\n",
      "epoch 13, time: 24s\n",
      "train loss: 3.0976\ttrain f1: 0.9552\t train acc: 0.9567\n",
      "  val loss: 1.1737\t  val f1: 0.8706\t   val acc: 0.8696\n",
      "batch sample weight: (array([0.93049115, 1.375348  ], dtype=float32), array([27,  5]))\n",
      "\n",
      "epoch 14, time: 24s\n",
      "train loss: 3.0934\ttrain f1: 0.9546\t train acc: 0.9562\n",
      "  val loss: 1.1727\t  val f1: 0.8709\t   val acc: 0.8705\n",
      "batch sample weight: (array([0.7974582, 1.1787134], dtype=float32), array([15, 17]))\n",
      "\n",
      "epoch 15, time: 24s\n",
      "train loss: 3.0821\ttrain f1: 0.9604\t train acc: 0.9616\n",
      "  val loss: 1.1563\t  val f1: 0.8874\t   val acc: 0.8870\n",
      "batch sample weight: (array([0.8373644, 1.2376983], dtype=float32), array([19, 13]))\n",
      "\n",
      "epoch 16, time: 24s\n",
      "train loss: 3.0773\ttrain f1: 0.9646\t train acc: 0.9657\n",
      "  val loss: 1.1830\t  val f1: 0.8617\t   val acc: 0.8594\n",
      "batch sample weight: (array([0.84797287, 1.2533786 ], dtype=float32), array([20, 12]))\n",
      "\n",
      "epoch 17, time: 24s\n",
      "train loss: 3.0640\ttrain f1: 0.9672\t train acc: 0.9682\n",
      "  val loss: 1.1704\t  val f1: 0.8731\t   val acc: 0.8721\n",
      "batch sample weight: (array([0.81692415, 1.2074859 ], dtype=float32), array([17, 15]))\n",
      "\n",
      "epoch 18, time: 24s\n",
      "train loss: 3.0602\ttrain f1: 0.9694\t train acc: 0.9702\n",
      "  val loss: 1.1969\t  val f1: 0.8467\t   val acc: 0.8459\n",
      "batch sample weight: (array([0.8373644, 1.2376983], dtype=float32), array([19, 13]))\n",
      "\n",
      "epoch 19, time: 24s\n",
      "train loss: 3.0520\ttrain f1: 0.9705\t train acc: 0.9713\n",
      "  val loss: 1.1668\t  val f1: 0.8777\t   val acc: 0.8764\n",
      "batch sample weight: (array([0.84797287, 1.2533786 ], dtype=float32), array([20, 12]))\n",
      "\n",
      "epoch 20, time: 24s\n",
      "train loss: 3.0588\ttrain f1: 0.9682\t train acc: 0.9692\n",
      "  val loss: 1.1880\t  val f1: 0.8569\t   val acc: 0.8550\n",
      "STOPPED-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "load_n_train(\n",
    "    save_name='r1_auto_weight_gamma1',\n",
    "    lr=1e-3,\n",
    "    gamma=1.,\n",
    "              augmenter=augmenter,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 376952,
     "status": "ok",
     "timestamp": 1617469819926,
     "user": {
      "displayName": "Đức Anh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj__uwgJPnV_bbazx9cfsZZmzMDKRMuz1IsbXR7Sw=s64",
      "userId": "05901412776980253231"
     },
     "user_tz": -420
    },
    "id": "82u4l-j35aZm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO7p802Nkx9+SC8DoP9+AZv",
   "collapsed_sections": [],
   "name": "r1_train_6_valid_30_auto_weight.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
